{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRESSED / PLAY : autoencoder music recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There's this iconic [YouTube Video](https://youtu.be/F5WWyyYG018?si=irttbZ33tBXrvl6-&t=121) that mixes the sounds of Frank Oceans's Blonde with the collective visual art of Hayao Miyazaki and Katsuhiro Otomo's Akira. The result is a stunning collage of emotionsal sesnation that really brigges the gap between sight and sound. There's one particular section that is simply sublime: a little 5 second snippet of a glider flying of into a pink and neon blue sunset sky while *Pink + White* plays in the background; a moment of perfect synergy between sight and sound. It's a particularly well curated example of something I feel that we all experience on a regular basis: that sensation of the song in your ears being perfectly matched to the vision hitting your eyes, be it a vibrant sunset or the neon colors of downtown at night, the musicl and the visual enhancing each other in a perfect ourobors.\n",
    "\n",
    "![sublimeness right here](pinkwhite.png)\n",
    "\n",
    "PRESSED / PLAY is an exploration of this cross-modal connection, leveraging the power of deep learning and the vast musical landscape thar Spotify offers. At the heart of this project is the challenge: gven an image, can we deliver a playlist or a set of song selections to match the vibe of the image? Such a system isn't just tech innovation, but also a novel way for us to discover music, turning everyday visual experiences into personalized musical ones. As we delve deeper, we'll uncover the steps and considerations that went into bringing this idea to fruition. From data collection to the intricacies of model selection and finally integrating with Spotify's recommendation engine, this journey is as much about the technicalities as it is about the art of merging two sensory worlds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the problem statement - translating art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**\"how can we capture the essence of an image and translate it into a musical experience?\"**\n",
    "\n",
    "this problem is multifaceted:\n",
    "\n",
    "1. **cross-modal translation**: At its core, this is a cross-modal problem where the challenge lies in bridging two vastly different types of data – visual images and musical genres or attributes.\n",
    "\n",
    "2. **subjectivity of interpretation**: The interpretation of images is highly subjective. The same image might evoke feelings of joy for one person and nostalgia for another. The music that resonates with these emotions could differ drastically.\n",
    "\n",
    "4. **integration with existing systems**: Once we've predicted a genre or mood, how do we translate that into a tangible outcome for the user? In our case, this means creating a playlist of songs that aligns with the predicted genre or mood.\n",
    "\n",
    "5. **scalability and generalization**: The solution needs to be scalable, catering to a wide array of images and diverse user tastes. It shouldn't be limited by the dataset's initial constraints.\n",
    "\n",
    "my aim is a system that can accept any image as input and, by understanding its content and emotion, produce a curated playlist that musically resonates with the image not only a technical challenge an artistic one, opening up new avenues for personalized content discovery and a richer multimedia experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the stack\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import io\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import spotipy\n",
    "from spotipy import Spotify\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from inference.img import random_color_saturation_and_hue, random_brightness_and_contrast, random_gaussian_blur, random_gaussian_noise, random_horizontal_flip, random_rotation, random_sharpen, random_vertical_flip, random_zoom_and_crop\n",
    "from inference.img import download_image_bytes, distort_encode\n",
    "from inference.vae import VAEAttention, vae_loss, CustomDataset, image_transform, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_file(file_path):\n",
    "\tenv = {}\n",
    "\twith open(file_path, 'r') as env_file:\n",
    "\t\tfor line in env_file:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif not line:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tkey, value = line.split('=', 1)\n",
    "\t\t\tenv[key] = value.replace('\"', '')\n",
    "\t\t\tos.environ[key] = value\n",
    "\treturn env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = load_env_file('inference/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## data - collection and creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "before we could even get started determining what model and architecture on which to base our system, we face 2 main underlying issues:\n",
    "how exactly do we build our training data, and how do we use that to generate a prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the basic inference blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the Spotify API has our solution, coming with an inbuilt endpoint, accessible via `spotipy` through `sp.recommendations`, which accepts as input, up to 5 seeding genres, and a list of target attributes with which to generate the predictions. The attributes which I've chose to work with are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FEATURES = [\n",
    "\t'acousticness',\n",
    "\t'danceability',\n",
    "\t'energy',\n",
    "\t'instrumentalness',\n",
    "\t'liveness',\n",
    "\t'loudness',\n",
    "\t'speechiness',\n",
    "\t'valence'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are all relatively self-explanatory, aside from `valence`, which is essentially just a measure of how \"happy\" a song is, in simplest terms.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see this process in action! continuing to use _Pink + White_ as our example, lets use the `sp.recommendations` endpoint to find some songs that are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_manager = SpotifyClientCredentials(client_id=envs['SPOTIFY_CLIENT_ID'], client_secret=envs['SPOTIFY_CLIENT_SECRET'])\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Put Your Records On',\n",
       "  'https://open.spotify.com/track/2nGFzvICaeEWjIrBrL2RAx'),\n",
       " ('Baby I Need Your Loving',\n",
       "  'https://open.spotify.com/track/6ClsM1x4P327baDUXp2Dep'),\n",
       " ('Window Seat', 'https://open.spotify.com/track/74HYrIbnpc2xKCTenv5qKM'),\n",
       " ('Crush', 'https://open.spotify.com/track/3Txcx4jhuiTZSvhAL0WaRc'),\n",
       " ('My Girl', 'https://open.spotify.com/track/0Bd4F0Ybq3kkqj1NBS8AaY'),\n",
       " (\"Don't Take It Personal\",\n",
       "  'https://open.spotify.com/track/5rwV5yAoPLfIjCZ64jvC2A'),\n",
       " ('1 Thing', 'https://open.spotify.com/track/1mnqraQ8oV8MX92rdOFLWW'),\n",
       " ('Fuck You', 'https://open.spotify.com/track/4ycLiPVzE5KamivXrAzGFG'),\n",
       " (\"Don't Stop 'Til You Get Enough\",\n",
       "  'https://open.spotify.com/track/46eu3SBuFCXWsPT39Yg3tJ'),\n",
       " ('Lil Bebe', 'https://open.spotify.com/track/7esO3L3DP7bM2OOd0Rdb4W')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_id = \"3xKsf9qdS1CyvXSMEid6g8\"\n",
    "track_details = sp.track(track_id)\n",
    "artist_id = track_details['artists'][0]['id']\n",
    "artist_genres = ['r-n-b', 'soul']\n",
    "audio_features_result = sp.audio_features([track_id])[0]\n",
    "target_features = {feature: audio_features_result[feature] for feature in AUDIO_FEATURES}\n",
    "recommendations = sp.recommendations(seed_genres=artist_genres, target_audio_features=target_features, limit=10)\n",
    "recommended_songs = [(track['name'], track['external_urls']['spotify']) for track in recommendations['tracks']]\n",
    "recommended_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and thats it! the recommendation algortihm by itself is easy enough to use, but the problem is how we retrieve the target audio features from an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem. Unlike other kinds od sandbox data science projects, there are no publicly available out-of-the-box dataset we could use to drive our model. We need an extensively labelled collection of images with audio features derived from songs, but that's not something that currenty exists on the internet. So let's make one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.collector import fetch_covers, fetch_track_ids, fetch_tracklist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imported are the functions that I built to accomplish, with names that are exactly as they say on the tin\n",
    "\n",
    "- `fetch_coers` gets all the playlist `id`s and cover images of a particular user; which we by default set to `'spotify'`\n",
    "- `fetch_track_ids` gets all the track `id`s in a particular playlist, which we use to get all the track`id`s for the dataset\n",
    "- `fetch_tracklist_data`: gets all the audio features we want from a track, applied over all the tracks we've obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user='spotify', n=float('inf')):\n",
    "\tcovers = fetch_covers(user=user, n=n)\n",
    "\tpl_ids, covers = (zip(*covers))\n",
    "\ttracklists = [fetch_track_ids(pl_id) for pl_id in tqdm(pl_ids)]\n",
    "\tfeatures = [fetch_tracklist_data(t, i) if t else None for i, t in tqdm(enumerate(tracklists))]\n",
    "\ttargets = [[list(map(track.get, AUDIO_FEATURES)) if track else None for track in tracklist] if tracklist else None for tracklist in tqdm(features)]\n",
    "\treturn covers, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:54<00:00,  1.74it/s]\n",
      "200it [00:54,  3.68it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3078.77it/s]\n"
     ]
    }
   ],
   "source": [
    "covers, targets = get_data(user='spotify', n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the returned objects are a list of urls linking to cover images and a list (of lists) of audio features. We have 2 main possible approahces from here: we could take the average of the audio features of each list, giving us a one to (many) relationship from picture to audio features, or we could train the same image to each song individually. \n",
    "\n",
    "The second approach would allow us to let the model fit to a more diverse set of obserations, which would allow it to generalize better, but we would then be training the model to associate the same image with a number of different audio features. To solve this problem, we can apply a set of random transformations to each image, and thus essentially ensure that every song is associated with a _unique_ image, introducing diversity to the dataset and allowing us to bootstrap outselves out of what otherwise would have been a 200 item training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortion_functions = [\n",
    "    random_rotation,\n",
    "    random_horizontal_flip,\n",
    "    random_vertical_flip,\n",
    "    random_zoom_and_crop,\n",
    "    random_brightness_and_contrast,\n",
    "    random_color_saturation_and_hue,\n",
    "    random_gaussian_noise,\n",
    "    random_gaussian_blur,\n",
    "    random_sharpen,\n",
    "]\n",
    "\n",
    "def apply_random_distortions(image):\n",
    "    distorted_image = image.copy()\n",
    "    transformation_mask = np.random.rand(len(distortion_functions)) > 0.5\n",
    "    for distortion, apply_distortion in zip(distortion_functions, transformation_mask):\n",
    "        if apply_distortion:\n",
    "            distorted_image = distortion(distorted_image)\n",
    "    return distorted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, I opt to use `dask`, since stitching together the data is an extremely computationaly intensive operation and `dask` helps to speed it up by an order of magnitude.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "@delayed\n",
    "def process_data(cover_url, tracklist):\n",
    "    data = []\n",
    "    if tracklist and cover_url:\n",
    "        image_bytes = download_image_bytes(cover_url)\n",
    "        if image_bytes:\n",
    "            for track_features in tracklist:\n",
    "                if track_features:\n",
    "                    image = distort_encode(image_bytes)\n",
    "                    if image:\n",
    "                        data.append((image, np.array(track_features)))\n",
    "    return pd.DataFrame(data, columns=['cover', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 388.22 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cover</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.15, 0.787, 0.621, 0.000402, 0.58, -5.009, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.256, 0.75, 0.733, 0.0, 0.114, -3.18, 0.0319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.269, 0.868, 0.538, 3.34e-06, 0.0901, -8.603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.701, 0.628, 0.523, 0.00274, 0.219, -8.307, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.0856, 0.673, 0.722, 0.0, 0.137, -3.495, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cover  \\\n",
       "0  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "1  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "2  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "3  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "4  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "\n",
       "                                            features  \n",
       "0  [0.15, 0.787, 0.621, 0.000402, 0.58, -5.009, 0...  \n",
       "1  [0.256, 0.75, 0.733, 0.0, 0.114, -3.18, 0.0319...  \n",
       "2  [0.269, 0.868, 0.538, 3.34e-06, 0.0901, -8.603...  \n",
       "3  [0.701, 0.628, 0.523, 0.00274, 0.219, -8.307, ...  \n",
       "4  [0.0856, 0.673, 0.722, 0.0, 0.137, -3.495, 0.0...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covers, targets\n",
    "df = dd.from_pandas(pd.DataFrame(columns=['cover', 'features']), npartitions=6)\n",
    "dfs = [process_data(cover, tracklist) for cover, tracklist in zip(covers, targets)]\n",
    "df = dd.from_delayed(dfs)\n",
    "with ProgressBar():\n",
    "\tdf = df.compute()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cover</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.15, 0.787, 0.621, 0.000402, 0.58, -5.009, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.256, 0.75, 0.733, 0.0, 0.114, -3.18, 0.0319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.269, 0.868, 0.538, 3.34e-06, 0.0901, -8.603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.701, 0.628, 0.523, 0.00274, 0.219, -8.307, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.0856, 0.673, 0.722, 0.0, 0.137, -3.495, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.0639, 0.485, 0.943, 0.179, 0.144, -4.423, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.221, 0.33, 0.895, 0.737, 0.408, -3.435, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.0148, 0.679, 0.697, 0.0119, 0.124, -5.369, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.296, 0.671, 0.725, 0.00022, 0.0889, -10.065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>[0.6, 0.618, 0.678, 0.723, 0.501, -5.199, 0.03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cover  \\\n",
       "0   b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "1   b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "2   b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "3   b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "4   b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "..                                                ...   \n",
       "46  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "47  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "48  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "49  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "50  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "\n",
       "                                             features  \n",
       "0   [0.15, 0.787, 0.621, 0.000402, 0.58, -5.009, 0...  \n",
       "1   [0.256, 0.75, 0.733, 0.0, 0.114, -3.18, 0.0319...  \n",
       "2   [0.269, 0.868, 0.538, 3.34e-06, 0.0901, -8.603...  \n",
       "3   [0.701, 0.628, 0.523, 0.00274, 0.219, -8.307, ...  \n",
       "4   [0.0856, 0.673, 0.722, 0.0, 0.137, -3.495, 0.0...  \n",
       "..                                                ...  \n",
       "46  [0.0639, 0.485, 0.943, 0.179, 0.144, -4.423, 0...  \n",
       "47  [0.221, 0.33, 0.895, 0.737, 0.408, -3.435, 0.0...  \n",
       "48  [0.0148, 0.679, 0.697, 0.0119, 0.124, -5.369, ...  \n",
       "49  [0.296, 0.671, 0.725, 0.00022, 0.0889, -10.065...  \n",
       "50  [0.6, 0.618, 0.678, 0.723, 0.501, -5.199, 0.03...  \n",
       "\n",
       "[14403 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is! We've turned a 200 item dataset into over 14000 items. Now that we have our data, we can now work on the model that will bridge the gap between images and the audio features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the model - intution and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE with attention: the intution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) and attention mechanisms are separately very poewrful in various applications. In our context, this combination architecture is especially robust for cross-model inference: let's explore why\n",
    "\n",
    "1. **data compression**: At the core of VAEs is the capability to compress complex data into a concise latent representation. This compression helps in capturing the essence of data like images or songs without much loss of information.\n",
    "2. **generative tendencies**: VAEs not only encode data but can also generate new, similar data. This generative ability aids tasks like recommendation.\n",
    "3. **attention's flexibility**: Attention mechanisms allow models to focus variably on different parts of the input. This dynamic focus means that the model can weigh parts of an image or song differently based on their relevance.\n",
    "4. **interpretability**: One of the allures of attention mechanisms is the interpretability it brings. It offers insights into what regions of input data the model deems important.\n",
    "5. **cross-modal translation**: Combining VAEs and attention provides a robust framework for translating information across different modalities, like images to music genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer-by-layer deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. encoder\n",
    "The encoder's job in a VAE is to take input data and produce a latent representation. This latent space captures the essential features of the data.\n",
    "\n",
    "Given input $ x $, the encoder outputs two vectors: mean $ \\mu $ and variance $ \\sigma^2 $. These vectors define a Gaussian distribution from which we can sample latent variables.\n",
    "\n",
    "$$\n",
    "\\mu, \\log \\sigma^2 = \\text{Encoder}(x)\n",
    "$$\n",
    "\n",
    "### 2. sampling\n",
    "\n",
    "Using the mean and variance from the encoder, the VAE samples a point in the latent space. This sampling introduces the stochastic element of the VAE.\n",
    "\n",
    "We sample $ z $ from the Gaussian distribution:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\odot \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is a random normal variable, and $ \\odot $ denotes element-wise multiplication.\n",
    "\n",
    "### 3. decoder\n",
    "\n",
    "The decoder takes the sampled latent variable $ z $ and attempts to reconstruct the original input: $ \\hat{x} $, the reconstructed input from the decoder.\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\text{Decoder}(z)\n",
    "$$\n",
    "\n",
    "### 4. attention mechanism\n",
    "\n",
    "Embedded within the VAE architecture, the attention mechanism allows the model to focus on specific parts of the input when constructing the latent representation. Given an input sequence $ x_1, x_2, ... x_n $, the attention scores for each element are computed as:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{\\exp(\\text{score}(x_i, z))}{\\sum_{j=1}^{n} \\exp(\\text{score}(x_j, z))}\n",
    "$$\n",
    "\n",
    "The final context vector, which is a weighted sum of the input elements, is:\n",
    "\n",
    "$$\n",
    "c = \\sum_{i=1}^{n} \\alpha_i x_i\n",
    "$$\n",
    "\n",
    "### 5. loss function\n",
    "\n",
    "The VAE has a unique loss function comprising two parts: reconstruction loss and KL divergence.\n",
    "\n",
    "- **reconstruction loss**: measures the difference between the original input and its reconstruction.\n",
    "  \n",
    "$$\n",
    "\\mathcal{L}_{recon} = \\| x - \\hat{x} \\|^2\n",
    "$$\n",
    "\n",
    "- **kl divergence**: measures the difference between the encoded distribution and a standard normal distribution. it acts as a regularizer.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{KL} = -0.5 \\sum_{i=1}^{n} (1 + \\log \\sigma^2 - \\mu^2 - \\sigma^2)\n",
    "$$\n",
    "\n",
    "The total loss is the sum of these two:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{recon} + \\mathcal{L}_{KL}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing the layers and mechanisms mentioned in `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEAttention(nn.Module):\n",
    "\tdef __init__(self, image_size=(3, 128, 128), audio_feature_size=8, latent_dim=32):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.hidden_size = 128 * (image_size[1] // 8) * (image_size[2] // 8)\n",
    "\n",
    "\t\tself.attention = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(128, 1, kernel_size=1),\n",
    "\t\t\tnn.Softmax(dim=2)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.fc_mu = nn.Linear(self.hidden_size, latent_dim)\n",
    "\n",
    "\t\tself.fc_logvar = nn.Linear(self.hidden_size, latent_dim)\n",
    "\n",
    "\t\tself.decoder = nn.Linear(latent_dim, audio_feature_size)\n",
    "\n",
    "\tdef encode(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tattention_weights = self.attention(x)\n",
    "\t\tx = x * attention_weights\n",
    "\t\tx = x.view(x.size(0), -1)\n",
    "\t\tmu = self.fc_mu(x)\n",
    "\t\tlogvar = self.fc_logvar(x)\n",
    "\t\treturn mu, logvar\n",
    "\n",
    "\tdef reparameterize(self, mu, logvar):\n",
    "\t\tepsilon = torch.randn_like(mu)\n",
    "\t\tz = mu + epsilon * torch.exp(0.5 * logvar)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef decode(self, z):\n",
    "\t\treturn self.decoder(z)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmu, logvar = self.encode(x)\n",
    "\t\tz = self.reparameterize(mu, logvar)\n",
    "\t\trecon_features = self.decode(z)\n",
    "\t\treturn recon_features, mu, logvar\n",
    "\n",
    "def vae_loss(recon_features, features, mu, logvar):\n",
    "\treconstruction_loss = F.mse_loss(recon_features, features, reduction='sum')\n",
    "\tkl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\ttotal_loss = reconstruction_loss + kl_divergence_loss\n",
    "\treturn total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, data, transform=None):\n",
    "\t\tself.data = data\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_bytes, audio_features = self.data.iloc[idx]['cover'], self.data.iloc[idx]['features']\n",
    "\t\timage = Image.open(io.BytesIO(image_bytes))\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\t\taudio = torch.tensor(audio_features, dtype=torch.float32)\n",
    "\t\treturn image, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "\ttransforms.Resize((128, 128)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train this model, I didn't do it locally on my own machine, since my resulting dataset was pretty big (76000) rows and the training would've been much more efficient on a GPU, which I don't have. Sadge. To get around this, I ran uploaded the code to and ran the training job on  AWS Sagemaker, whcih allowed me to run the training in the cloud on GPU, also allowing me to integrate S3 into how I served the model in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's the script for how I engineered that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is 'train.py'\n",
    "def train(epochs, model_dir, train_data_directory, debug=True):\n",
    "\tall_files = [os.path.join(train_data_directory, file) for file in os.listdir(train_data_directory) if file.endswith('.parquet')]\n",
    "\tcombined_df = pd.concat([pd.read_parquet(file) for file in all_files], ignore_index=True)\n",
    "\tif debug:\n",
    "\t\tcombined_df = combined_df[:1000]\n",
    "\tdf = combined_df\n",
    "\tprint('data successfully obtained')\n",
    "\n",
    "\tdataset = CustomDataset(df, transform=image_transform)\n",
    "\tdataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\tmodel = VAEAttention().to(device)\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\t\n",
    "\tprint('training begin')\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor images, audio_features in dataloader:\n",
    "\t\t\timages = images.to(device)\n",
    "\t\t\taudio_features = audio_features.to(device)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\trecon_audio_features, mu, logvar = model(images)\n",
    "\t\t\tloss = vae_loss(recon_audio_features, audio_features, mu, logvar)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\n",
    "\t\tprint(f\"epoch {epoch+1}/{epochs}, loss: {loss.item():.4f}\")\n",
    "\t\ttorch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n",
    "\t\n",
    "\ttorch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    args = parser.parse_args()\n",
    "    train(args.epochs, args.model_dir, args.train, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sagemaker_train():\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        region_name='us-east-2'\n",
    "    )\n",
    "    sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "    role = os.environ('SAGEMAKER_ROLE')\n",
    "\n",
    "    source_dir = os.path.basename(os.path.dirname(os.path.abspath(__file__)))\n",
    "    code_location = 's3://coverdata/model_train'\n",
    "\n",
    "    estimator = PyTorch(\n",
    "        entry_point='train.py',\n",
    "        source_dir=f\"../{source_dir}/\",\n",
    "        code_location=code_location,\n",
    "        role=role,\n",
    "        framework_version='1.8.1',\n",
    "        py_version='py3',\n",
    "        instance_count=1,\n",
    "        instance_type='ml.g4dn.2xlarge',\n",
    "        hyperparameters={},\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    training_data_channel = sagemaker.inputs.TrainingInput(\n",
    "        s3_data='s3://coverdata/data/train', \n",
    "        content_type='parquet'\n",
    "    )\n",
    "    estimator.fit({'train': training_data_channel})\n",
    "    return estimator\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sagemaker_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training finishes successfully, the model is deployed as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def model_fn(access_id, access_key):\n",
    "\tmodel_pth = maintain_pth(access_id, access_key)\n",
    "\tmodel = VAEAttention().to(device)\n",
    "\tmodel.load_state_dict(torch.load(model_pth, map_location=device))\n",
    "\tmodel.eval()\n",
    "\tprint('model init complete')\n",
    "\treturn model\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "\timage = Image.open(BytesIO(input_data))\n",
    "\timg_tensor = image_transform(image).unsqueeze(0).to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\tfeatures = model(img_tensor)[0]\n",
    "\toutput = features.tolist()[0]\n",
    "\treturn output\n",
    "\n",
    "def maintain_pth(access_id, access_key):\n",
    "\tsession = boto3.Session(\n",
    "\t\taws_access_key_id=access_id,\n",
    "\t\taws_secret_access_key=access_key,\n",
    "\t\tregion_name='us-east-2'\n",
    "\t)\n",
    "\ts3 = session.resource('s3')\n",
    "\ts3_uri = 's3://coverdata/model/model.pth'\n",
    "\tbucket, model_path = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\tlocal_path = '../inference/model.pth'\n",
    "\ts3.Bucket(bucket).download_file(model_path, local_path)\n",
    "\treturn '../inference/model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## genre-lization - the last puzzle pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've collected our data, trained a model, and are now ready to make our predictions right? We have one last step. Remember how we needed to supply the target features (which we can predict with our model now, wow!) and also up to 5 genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all of a user's genres ia relatively trivial. I just take all of their top artists, and aggregate the genres of those artists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genremappings.json', 'r') as file:\n",
    "    genre_mappings = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotifyClient:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.sp = Spotify(auth=token)\n",
    "        self._top_artists_cache = {}\n",
    "        self._genres_cache = None\n",
    "\n",
    "    def get_top_artists(self, time_range):\n",
    "        if time_range not in self._top_artists_cache:\n",
    "            self._top_artists_cache[time_range] = self.sp.current_user_top_artists(limit=50, time_range=time_range)['items']\n",
    "        return self._top_artists_cache[time_range]\n",
    "\n",
    "    def map_genres(self):\n",
    "        if self._genres_cache is None:\n",
    "            all_genres = []\n",
    "            seen_artist_ids = set()\n",
    "            for term in ['short_term', 'medium_term', 'long_term']:\n",
    "                artists = self.get_top_artists(term)\n",
    "                for artist in artists:\n",
    "                    if artist['id'] not in seen_artist_ids:\n",
    "                        seen_artist_ids.add(artist['id'])\n",
    "                        all_genres.extend(artist.get('genres', []))\n",
    "            mappings = []\n",
    "            for genre in all_genres:\n",
    "                if genre in genre_mappings.keys():\n",
    "                    mappings.extend(genre_mappings[genre])\n",
    "            self._genres_cache = mappings\n",
    "        return self._genres_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might immediately notice some things here that stand out. What are genre mappings? Why would a genre need to be mapped?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat of using Spotify's `sp.recommendations` framework is Spotify only allows seeding for a very small and narrow subet of genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': ['acoustic',\n",
       "  'afrobeat',\n",
       "  'alt-rock',\n",
       "  'alternative',\n",
       "  'ambient',\n",
       "  'anime',\n",
       "  'black-metal',\n",
       "  'bluegrass',\n",
       "  'blues',\n",
       "  'bossanova',\n",
       "  'brazil',\n",
       "  'breakbeat',\n",
       "  'british',\n",
       "  'cantopop',\n",
       "  'chicago-house',\n",
       "  'children',\n",
       "  'chill',\n",
       "  'classical',\n",
       "  'club',\n",
       "  'comedy',\n",
       "  'country',\n",
       "  'dance',\n",
       "  'dancehall',\n",
       "  'death-metal',\n",
       "  'deep-house',\n",
       "  'detroit-techno',\n",
       "  'disco',\n",
       "  'disney',\n",
       "  'drum-and-bass',\n",
       "  'dub',\n",
       "  'dubstep',\n",
       "  'edm',\n",
       "  'electro',\n",
       "  'electronic',\n",
       "  'emo',\n",
       "  'folk',\n",
       "  'forro',\n",
       "  'french',\n",
       "  'funk',\n",
       "  'garage',\n",
       "  'german',\n",
       "  'gospel',\n",
       "  'goth',\n",
       "  'grindcore',\n",
       "  'groove',\n",
       "  'grunge',\n",
       "  'guitar',\n",
       "  'happy',\n",
       "  'hard-rock',\n",
       "  'hardcore',\n",
       "  'hardstyle',\n",
       "  'heavy-metal',\n",
       "  'hip-hop',\n",
       "  'holidays',\n",
       "  'honky-tonk',\n",
       "  'house',\n",
       "  'idm',\n",
       "  'indian',\n",
       "  'indie',\n",
       "  'indie-pop',\n",
       "  'industrial',\n",
       "  'iranian',\n",
       "  'j-dance',\n",
       "  'j-idol',\n",
       "  'j-pop',\n",
       "  'j-rock',\n",
       "  'jazz',\n",
       "  'k-pop',\n",
       "  'kids',\n",
       "  'latin',\n",
       "  'latino',\n",
       "  'malay',\n",
       "  'mandopop',\n",
       "  'metal',\n",
       "  'metal-misc',\n",
       "  'metalcore',\n",
       "  'minimal-techno',\n",
       "  'movies',\n",
       "  'mpb',\n",
       "  'new-age',\n",
       "  'new-release',\n",
       "  'opera',\n",
       "  'pagode',\n",
       "  'party',\n",
       "  'philippines-opm',\n",
       "  'piano',\n",
       "  'pop',\n",
       "  'pop-film',\n",
       "  'post-dubstep',\n",
       "  'power-pop',\n",
       "  'progressive-house',\n",
       "  'psych-rock',\n",
       "  'punk',\n",
       "  'punk-rock',\n",
       "  'r-n-b',\n",
       "  'rainy-day',\n",
       "  'reggae',\n",
       "  'reggaeton',\n",
       "  'road-trip',\n",
       "  'rock',\n",
       "  'rock-n-roll',\n",
       "  'rockabilly',\n",
       "  'romance',\n",
       "  'sad',\n",
       "  'salsa',\n",
       "  'samba',\n",
       "  'sertanejo',\n",
       "  'show-tunes',\n",
       "  'singer-songwriter',\n",
       "  'ska',\n",
       "  'sleep',\n",
       "  'songwriter',\n",
       "  'soul',\n",
       "  'soundtracks',\n",
       "  'spanish',\n",
       "  'study',\n",
       "  'summer',\n",
       "  'swedish',\n",
       "  'synth-pop',\n",
       "  'tango',\n",
       "  'techno',\n",
       "  'trance',\n",
       "  'trip-hop',\n",
       "  'turkish',\n",
       "  'work-out',\n",
       "  'world-music']}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.recommendation_genre_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the genres for aren't limited to that. In fact, [Every Noise at Once](https://everynoise.com/#otherthings) has amassed a staggering collection of every genre that has been encountered in the Spotify ecosystem. Herein laid a new problem: how do I map out all of these oer 5000 genres to its closest seedable genre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a number of methods, including word similarity metrics, Word2Vec / FastText similarity compariosn, but in the end, none of them contain the outsie contextual knowledge to properly perform a mapping like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'musica triste brasileira': ['brazil', 'sad']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As impressive as FastText is, unfortunately it can't do translation on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I used ChatGPT. And so the `genremappings.json` file was created, allowing us to properly best map our user's top genres to genres that are seedable by Spotify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# putting it all together\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of it all, here's the final code that's run in my Django app when a user uploads an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_image(request):\n",
    "\tclient = SpotifyClient(request.session['access_token'])\n",
    "\timage = request.FILES['image'].read()\n",
    "\tresult = predict_fn(image, model)\n",
    "\tprint(result)\n",
    "\tfeatures = dict(zip(TARGET_FEATURES, result))\n",
    "\tspotify_uri = client.create_playlist(request, features)\n",
    "\treturn JsonResponse({'spotifyURI': spotify_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an image is uploaded, an inference is made, a playlist is created, and we have a vibe. nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to get hands on with my toy, you can visit it [here](pressedplay.rdszhao.com) at pressedplay.rdszhao.com. You just need a Spotify account. If you're interested, email me at [rdszhao@gmail.com](rdszhao@gmail.com) to try it out. Thanks for reading, and cheers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
